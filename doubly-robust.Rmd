---
output: pdf_document
---

# Doubly Robust Estimation of Causal Effects

Welcome! This is a conversational introduction to doubly robust estimation. Largely following [[Funk et al. 2011]{.underline}](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3070495/), we’ll talk about what doubly robust estimation is and why we do it; break down a simple doubly robust estimator to understand how it works; and run some simulations to see our estimator in action.

## What and why

I like to think of “causal inference” as a catch-all term for the tools we have to figure out whether relationships we see in the world are cause and effect or just coincidence. Often, we learn about these relationships by modeling them: we collect data on something we’re curious about and fit models to that data to help us see the patterns in it. Models capture patterns in the data in the form of numerical estimates.

But fitting models to data almost always requires making some assumptions about how the world works. For example, when we fit a linear regression model, we assume the relationship we are quantifying is linear. And when we misspecify our models – that is, when our assumptions about the world are wrong – the estimates our models produce are poor.

Doubly robust estimation is one way to help insure our estimates against bad assumptions. It combines two modeling strategies, propensity score estimation and outcome regression modeling, in such a way that as long as one of the two models is correct, our estimates will be unbiased – even if the other model is misspecified.

You can think of doubly robust estimation as statistics’ version of the [[Swiss cheese model]{.underline}](https://www.nytimes.com/2020/12/05/health/coronavirus-swiss-cheese-infection-mackay.html). Each modeling strategy – propensity score estimation and outcome regression modeling – is a layer of defense against bias. Neither is perfect, but by layering the strategies on top of each other, we give ourselves more opportunities to block bias from degrading our estimates.

## Some housekeeping

First things first, notation. I follow Funk et al., except that I use $D$ instead of $X$ to denote treatment, to avoid confusion given you’ll often see $X$ used elsewhere to denote covariates. Here is a full list of the terms we’re going to use:

$\bf{Z}$ are characteristics (that is, covariates) of individuals in our data

$D$ is a binary treatment indicator equal to $1$ if an individual is exposed to treatment or $0$ if they aren’t

$Y_{D=1}$ and $Y_{D=0}$ are *observed* outcomes for individuals in the treatment and control groups, respectively

$\hat{Y}_{D=1}$ and $\hat{Y}_{D=0}$ are *predicted* counterfactual outcomes under treatment and control, respectively [^1]

[^1]: Per usual, any term with a "hat" is an estimate of a true quantity we can't measure directly

$m1(\bf{Z_i}$$, \hat{\alpha}_1)$ and $m0(\bf{Z_i}$$, \hat{\alpha}_0)$ are the models we’ll use to predict $\hat{Y}_{D=1}$ and $\hat{Y}_{D=0}$ for each individual, where $\bf{Z_i}$ are that individual's characteristics and $\hat{\alpha}_1$ and $\hat{\alpha}_0$ are coefficients that describe the estimated relationships between characteristics and outcomes in the treatment and control groups; we get $\hat{\alpha}_1$ and $\hat{\alpha}_0$ by fitting linear regression models that predict outcomes using characteristics

$PS=P[D=1|\bf{Z}]$ is the propensity score, which measures how likely an individual is to be exposed to treatment based on their characteristics

$e(\bf{Z_i}$$, \hat{\beta})$ is the model we'll use to estimate the propensity score, where $\bf{Z_i}$ are once again and individual's characteristics and $\hat{\beta}$ is a coefficient that describes the estimated relationship between characteristics and treatment propensity; we get $\hat{\beta}$ by fitting a logistic regression model that predicts treatment using characteristics

$DR_1$ and $DR_0$ are the doubly robust estimated outcomes under treatment and control

Also, a few assumptions. Briefly, **positivity** or overlap assumes every individual in the data had some chance of receiving treatment: formally, $0\leq P[D_i=1]\leq1$. It ensures we have individuals in the treatment and control groups to contrast. **Consistency** or the stable unit treatment assumption (SUTVA) assumes every individual has a single, stable potential outcome for each possible treatment: formally, Yi(d) = Yi if Di = d. SUTVA has several implications; see Anton Strezhnev’s [[introduction to the potential outcomes framework]{.underline}](https://github.com/UChicago-pol-methods/plsc-30600-causal-inference/blob/main/slides/week1/Week%201_%20Potential%20Outcomes.pdf) for an excellent summary. Finally, **ignorability**, unconfoundedness, or exchangeability assumes no selection-into-treatment bias. Put another way, it assumes treatment is independent of the potential outcomes. Ignorability allows us to assume the treatment and control groups are both representative of the sample as a whole, which means we can take quantities estimated on either group as representative of the sample. This is what allows us to interpret differences in outcomes between the treatment and control groups as average treatment effects, and it’s also key to understanding the doubly robust estimator we’re about to break down.

## A simple doubly robust estimator

```{r, echo=FALSE, message=FALSE}
library(tidyverse)
```

```{r}
# GENERATE "TRUE" DATE PER FUNK ET AL. 2011
n = 2000

# GENERATE IVs
z1 <- rnorm(n, mean = 0, sd = 1)
z2 <- rnorm(n, mean = 0, sd = 1)
z3 <- replicate(n, ifelse(rnorm(1, mean = 0, sd = 1) <= 0.3, 1, 0))

# GENERATE EXPOSURE AS A FUNCTION OF IVs

# Set betas for true model
b0_0 = 1.5
b1_0 = 1
b2_0 = -2
b3_0 = 1

# Calculate true propensity scores (logit)
ps <- (1 + exp(b0_0 + b1_0*z1 + b2_0*z2 + b3_0*z3))^(-1)

# Calculate exposure
d <- ifelse(ps + runif(n, min = 0, max = 1) < 0.91, 1, 0)

# GENERATE TRUE OUTCOMES
b4_0 <- 2
z4 <- rnorm(n, mean = 0, sd = 1)
y <- b1_0*z1 + b3_0*z3 + b4_0*z4

# TRUE DATA
data <- tibble(z1, z2, z3, d, ps, y)
```

**TODO:** Rephrase Funk et al. excerpts (block quotes) in my own words

> The doubly robust estimator requires us to specify regression models for the outcome and the exposure as a function of covariates. In the case of this particular doubly robust estimator, we model the relations between confounders and the outcome within each exposure group. 

```{r}
# SCENARIO 1
# Fit outcome regression models
modt_1 <- lm(y ~ z1 + z3, data = data %>% filter(d == 1))
modc_1 <- lm(y ~ z1 + z3, data = data %>% filter(d == 0))
```

 > The resulting parameter estimates are used to calculate the predicted response ($\hat{Y_0}$ and $\hat{Y_1}$) for each individual in the population under the 2 exposure conditions ($D=1$ and $D=0$ given covariate values ($\bf{Z}$). 
 
```{r}
# SCENARIO 1
# Predict outcomes
data <- data %>% mutate(yhat1_1 = predict(modt_1, data))
data <- data %>% mutate(yhat0_1 = predict(modc_1, data))
```
 
> In addition, we model the exposure as a function of covariates to estimate the PS (or predicted probability of exposure conditional on covariates, $\bf{Z}$) for each individual using the observed data. These quantities are all subject specific, but we have omitted the additional subscript ($i$) for readability. 

```{r}
# SCENARIO 1

# Fit propensity score model
ps_1 <- glm(d ~ z1 + z3, data = data, family = 'binomial') # Logit

# Predict propensity scores
data <- data %>% mutate(ps1 = predict(ps_1, data))
```

> Having estimated the PS,$\hat{Y_0}$ and $\hat{Y_1}$, we combine these values as shown in Table 1 to calculate the doubly robust ($DR$) estimates of response in the presence and absence of exposure ($DR_1$ and $DR_0$, respectively) for each individual.

```{=tex}
\begin{table}[h]
\begin{tabular}{llll}
& $DR_1$ & $DR_0$ &  \\
For $D=1$ & $\frac{Y_{D=1}}{PS}-\frac{\hat{Y_1}\left(1-PS\right)}{PS}$ & $\hat{Y_0}$ & \\
For $D=0$ & $\hat{Y_1}$ & $\frac{Y_{D=0}}{1-PS}-\frac{\hat{Y_0}\left(PS\right)}{1-PS}$ & 
\end{tabular}
\end{table}
```

```{r}
# SCENARIO 1

# Compute doubly robust estimates
data <- data %>% mutate(dr1_1 = 
                          ifelse(d == 1, 
                                 ((y / ps1) - ((yhat1_1 * (1 - ps1)) / (ps1))),
                                 (yhat1_1)),
                        dr1_0 =
                          ifelse(d == 1,
                                 (yhat0_1),
                                 ((y / (1 - ps1) - ((yhat0_1 * ps1) / (1 - ps1))))))

```

> Finally, the means of $DR_1$ and $DR_0$ are calculated across the entire study population. The estimated means are used to calculate the difference or ratio effect measure (Funk et al. pp. 762-763).

```{r}
# SCENARIO 1

# Compute ATE
(mean(data$dr1_1) - mean(data$dr1_0))
```

**TODO:** Rephrase this in my own words

>  Closer examination of the equation for this doubly robust estimator suggests an intuitive explanation of the doubly robust property. With minor manipulation, it can be represented as an estimator for the quantity of interest (the mean response if everyone had been exposed/unexposed) plus a second term referred to as the "augmentation." This component is formed by taking the product of 2 bias terms—one from the PS model and one from the outcome regression model. If either bias term equals zero (as is the case when one of the models is correct), then it "zeros out" the other, nonzero bias term from the incorrect model. Thus, if either the PS or the outcome regression models are correctly specified, then the "augmentation" term reduces to zero so that $DR_1$ estimates $E[Y_{D=1}]$ and, likewise, $DR_0$ estimates $E[X_{Y=0}]$.

---
## All code in one place

```{r}
# GENERATE ESTIMATED OUTCOMES

# Scenario 1

# Fit outcome regression models
modt_1 <- lm(y ~ z1 + z3, data = data %>% filter(d == 1))
modc_1 <- lm(y ~ z1 + z3, data = data %>% filter(d == 0))

# Predict outcomes
data <- data %>% mutate(yhat1_1 = predict(modt_1, data))
data <- data %>% mutate(yhat0_1 = predict(modc_1, data))

# Fit propensity score model
ps_1 <- glm(d ~ z1 + z3, data = data, family = 'binomial') # Logit

# Predict propensity scores
data <- data %>% mutate(ps1 = predict(ps_1, data))

# Compute doubly robust estimates
data <- data %>% mutate(dr1_1 = 
                          ifelse(d == 1, 
                                 ((y / ps1) - ((yhat1_1 * (1 - ps1)) / (ps1))),
                                 (yhat1_1)),
                        dr1_0 =
                          ifelse(d == 1,
                                 (yhat0_1),
                                 ((y / (1 - ps1) - ((yhat0_1 * ps1) / (1 - ps1))))))

# Compute ATE
(mean(data$dr1_1) - mean(data$dr1_0))
```

```{r}
# Ari's debugging code
(mean(data$yhat1_1 - data$yhat0_1))
(mean((d/data$ps1 - (1-d)/data$ps1) * data$y))
(mean(data$dr1_1) - mean(data$dr1_0))
```

